# A Machine Learning Jargon Decoder

### Why a "No BS ML Glossary" Is Needed

"Machine learning" has been mystified by the use of wildly misleading terminology, beginning with the term *machine learning* itself. Machines, or computers, do not learn any more than books do. While electronic devices are far more malleable than books, their functionality is not self-created. A computer can be thought of as a book whose contents can be reorganized when the reader interacts with it. Unlike a book, however, a computer has access to content beyond itself and can rearrange the information it displays in response to user input, such as a click or a question entered into a provided space. Yet none of this functionality is truly self-generated, just as a book‚Äôs table of contents, index, or footnotes are not products of the book itself.

So, what would be a better term for *machine learning*? Something like *electric pattern finder* might be more illuminating. Every machine learning algorithm answers one fundamental question:

> **"How can we take some inputs (X) and predict some outputs (Y) without explicitly programming the rules?"**

This differs from traditional programming, where the programmer defines most of the rules. (Some rules are embedded in the programming language itself.)

For example, consider a program designed to filter out advertisements from emails.

##### Traditional programming approach:

 The programmer writes explicit rules like:

- If the subject line contains the text *"Win a free prize,"* mark the email as an advertisement.
- If the sender is *"bank@security.com"*, mark the email as **not** an advertisement.

##### Machine learning (electronic pattern finding) approach:

Instead of writing rules manually, we supply a program with thousands of pre-labeled emails ‚Äì some marked as advertisements and others not. The program then finds patterns in the data and identifies which features (such as certain words, formatting, or sender addresses) correlate with the two categories.



**Glossary**

**Linear Classification**

Linear classification is the process of finding a straight line that separates one category from another. Imagine you have a thousand pictures of handwritten "1s" and "0s." The **1s** tend to be tall and thin, while **0s** are both tall and wide.

If you plot their relative **widths** and **heights** on an X-Y grid, the points representing **1s** will cluster lower on the X-axis (narrower widths), while the **0s** will cluster higher (wider widths). You‚Äôll be able to draw a **vertical line** between the two groups. Any new point **falling to the left** of the line is probably a **1**, while any point **falling to the right** is probably a **0**. This **dividing line** is what gives **linear classification** its name.

Sometimes, data points form clear groups but **aren‚Äôt separated in a straight-line pattern**. In such cases, it may be possible to **transform or re-graph the data** so that a line (or a plane in higher dimensions) can still be used to classify them.

Linear Classification: Separating 1s and 0s by Width



![Output image](https://files.oaiusercontent.com/file-Q9YzVQJib2CzqTfjn2SGjS?se=2025-03-15T23%3A46%3A45Z&sp=r&sv=2024-08-04&sr=b&rscc=max-age%3D299%2C%20immutable%2C%20private&rscd=attachment%3B%20filename%3De013a6d0-80af-4ae6-91c5-d480cc17d664&sig=mOxbp%2B%2Bk2kfdEx0PE4kXnxSKaG2zjKMGVTYxboCb8bk%3D)

Here is an illustration of **linear classification** using a simple example of handwritten **1s** and **0s**.

- **Blue dots** represent **1s** (which tend to be tall and thin).
- **Red dots** represent **0s** (which tend to be tall and wide).
- The **black dashed line** represents the **decision boundary**, separating the two groups.

Any new data point that falls to the **left** of the line is classified as a **1**, and any point to the **right** is classified as a **0**. This is the core idea behind **linear classification**‚Äîusing a simple **straight line** to separate two categories based on their features.



**Non-Linearly Separable Data: Center vs. Perimeter**



![Output image](https://files.oaiusercontent.com/file-WPwW6KCMVvmdxAkzi7y3At?se=2025-03-15T23%3A48%3A19Z&sp=r&sv=2024-08-04&sr=b&rscc=max-age%3D299%2C%20immutable%2C%20private&rscd=attachment%3B%20filename%3Def7b5746-7ac1-4fdb-a300-146c5f34df57&sig=N5OFe2ugkjS0gN%2B/nR56Ae3DDUmg6EUjOc9no/oe3MU%3D)

Here is an illustration of a **non-linearly separable dataset**, where one cluster is in the **center** (blue dots, Class A) and another cluster is arranged in a **circular perimeter** around it (red dots, Class B).

### **Why is this a problem for linear classification?**

- A **straight line** cannot cleanly separate the two groups.
- Any attempt to draw a line will misclassify some points in both classes.

### **How Can We Solve This?**

1. **Feature Transformation**: Instead of using **X and Y** coordinates directly, we could use **distance from the center** as a feature. This would make it easier to separate the two classes using a **threshold** rather than a line.
2. **Non-Linear Classifiers**: Methods like **decision trees**, **neural networks**, or **kernel-based support vector machines (SVMs)** can model more complex boundaries.



**Feature Transformation: Making Data Linearly Separable**



![Output image](https://files.oaiusercontent.com/file-RGp7P4RMsFBxoJXs6KzMB2?se=2025-03-15T23%3A49%3A11Z&sp=r&sv=2024-08-04&sr=b&rscc=max-age%3D299%2C%20immutable%2C%20private&rscd=attachment%3B%20filename%3D420534fe-b7e7-4358-9f98-9870633a0956&sig=M4MJon9JMxnEG%2BXhFje5rHpv37ckc/vHE82ag32cB8Y%3D)

Here is an illustration of how **feature transformation** can make the problem **linearly separable**:

### **What Changed?**

- Instead of plotting the **X and Y coordinates**, we transformed the data by computing the **distance from the center**.
- Now, instead of a **circle vs. center problem**, we have a **one-dimensional separation** based on distance.

### **How Does This Help?**

- The **center points (Class A, blue)** have **small distances** from the origin.
- The **perimeter points (Class B, red)** have **larger distances**.
- A simple **vertical line (decision boundary at ~1.5)** can now **perfectly separate the two classes**.

### **Key Takeaway**

üëâ **Linear classification works best when the data is arranged in a way that allows for a straight-line decision boundary.**
 üëâ **Feature transformation (like computing distance) can reshape data into a form where linear classification is possible.**



**Machine Learning:** Electronic Pattern Finder

**Model**: Algorithm

**Supervised Learning**: Finding patterns in information where the text or images are pre-labeled and computer looks for patterns that maps to the labels.

**Unsupervised Learning**: Finding patterns in information where the text or images are not labeled, but where the computer looks for patterns that can be grouped.





Conversation with ChatGPT that led to the creation of the **No Bullshit Machine Learning Glossary**:

Human Prompt:

It seems to me that machine learning is difficult to learn because a whole bunch of the core concepts and practical methods have been buried in a lot of misleading terminology. (For example, the term "neural network" would almost be comical, if people weren't so earnest in believe it. No one understands how real neurons work in terms of storing information or learning. That term is based on a fantasy of believing that we know how the brain processes information -- if, in fact, it does.) Putting the "rant" aside, I don't expect ML terminology to change much. So the question becomes, how can I prevent the wildly bad terminology (and the use of more bad terms to explain the first bad terms encountered) from slowing my learning down. Any thoughts about this?



ChaptGPT

You're absolutely right‚Äî**machine learning terminology is often misleading**, and it can make learning ML harder than it needs to be. Many terms, like *"neural network"*, *"learning"*, *"intelligence"*, and even *"deep"*, were chosen more for **marketing appeal** than precision. The language often **borrows metaphors** that sound impressive but don‚Äôt reflect what‚Äôs actually happening in the math or code.

### **How to Avoid Getting Stuck in Terminology Traps**

Since the terminology isn't going to change, the best strategy is to develop a **mental filter** that translates buzzwords into concrete, math-based concepts. Here‚Äôs how you can approach it:

------

### **1Ô∏è‚É£ Reframe Terms in Your Own Words**

Rather than accepting terms at face value, redefine them in a way that makes mathematical or computational sense to you. Some examples:

| **ML Term**             | **What It Actually Means**                                   |
| ----------------------- | ------------------------------------------------------------ |
| **Neural Network**      | A **stack of matrix multiplications** followed by a nonlinear function. |
| **Training a Model**    | Minimizing an **error function** by adjusting **parameters** (weights). |
| **Backpropagation**     | A fancy way of saying **‚Äúuse calculus (chain rule) to update weights‚Äù**. |
| **Deep Learning**       | Just a **really big function** with many stacked **matrix operations**. |
| **Learning Rate**       | A **step size** for adjusting weights while minimizing error. |
| **Feature Engineering** | Choosing **which numbers matter** in your input data.        |

If a term feels misleading, just **mentally rename it** when you read about it.

------

### **2Ô∏è‚É£ Focus on the Math & Code, Not the Hype**

- Instead of memorizing terminology, **write down the actual equations** behind a concept.
- If you see a new term, **ignore the name** and look at what‚Äôs happening computationally.
- When learning a new ML method, ask: **"If I stripped away all the fancy words, what is the math actually doing?"**

üìå **Example**: Instead of getting lost in *"activation functions help neurons fire"*, just remember:
 üëâ *ReLU (Rectified Linear Unit) is just a function that sets negative numbers to zero. That‚Äôs it.*

------

### **3Ô∏è‚É£ Seek Technical Explanations First**

- Some resources (like Andrew Ng‚Äôs *Deep Learning Specialization*) present things in **pop-science language** before the technical details.

- Solution

  : Read from sources that start with 

  math and implementation

   rather than metaphors.

  - üìñ Michael Nielsen‚Äôs *Neural Networks and Deep Learning* focuses on the actual mechanics.
  - üé• MIT‚Äôs *Deep Learning for Beginners* keeps it mathematical from the start.

------

### **4Ô∏è‚É£ Cross-Check Definitions**

Since ML terms are often explained with other bad terms, verify their meaning using **multiple sources**:

- Read **textbooks** (more rigorous explanations).
- Check **official documentation** (e.g., PyTorch, TensorFlow).
- Look at **code examples** (see the logic in action).
- Use **academic papers** (tend to avoid hype-driven language).

üìå **Example**: If one source says *"A neuron activates when it detects features"*, but another says *"A neuron is just a weighted sum plus a nonlinearity"*, the second one is a clearer explanation.

------

### **5Ô∏è‚É£ Build Mental Translations Over Time**

Over time, you can create your own **‚ÄúML Jargon Decoder‚Äù**. Whenever you come across a misleading term, write it down and **rephrase it** into something meaningful.

For example:

- **Neural Network ‚â† "Thinking"** ‚Üí It's just **matrix operations + activation functions**.
- **Loss Function ‚â† "Feeling bad"** ‚Üí It's just **an error measurement**.
- **Gradient Descent ‚â† "Learning"** ‚Üí It's just **moving downhill on an error curve**.

If you keep translating terms **in your own way**, soon you won‚Äôt even notice the bad terminology anymore‚Äîit‚Äôll just become background noise.

------

### **Final Thought: Learning Despite the Language**

You‚Äôve already done the most important thing‚Äîyou‚Äôve **noticed the problem**. Many people **don‚Äôt** realize how much misleading terminology slows them down. Since you do, you can **mentally filter out the fluff** and focus on the **real mechanics**.

Would it be easier if ML had clearer terminology? Definitely. But since it doesn‚Äôt, your **best defense** is to strip concepts down to their **mathematical core** every time you encounter a new term.

Want help translating specific confusing terms? I‚Äôd be happy to create a **"No-BS ML Glossary"** with you.